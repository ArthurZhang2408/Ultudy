# ============================================================================
# ULTUDY BACKEND ENVIRONMENT CONFIGURATION
# ============================================================================
# Complete reference for all environment variables
# Last Updated: 2025-01-17
# See: backend/ENV_CONFIGURATION.md for detailed documentation

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================

# Server port
PORT=3001

# Node environment (development, production, test)
NODE_ENV=development

# Continuous Integration flag (automatically set by CI systems)
# CI=true

# Worker ID for multi-instance deployments (optional)
# WORKER_ID=1

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================

# Primary database connection URL
# Format: postgres://username:password@host:port/database
DATABASE_URL=postgres://postgres:postgres@localhost:5432/study_app

# Alternative: Individual connection parameters (used if DATABASE_URL not set)
# PGHOST=localhost
# PGPORT=5432
# PGUSER=postgres
# PGPASSWORD=postgres
# PGDATABASE=study_app

# Database Connection Pool Settings
# Increased for production scalability (supports ~1,000 concurrent users)
# For 10k+ users, add PgBouncer connection pooler in front of database
DB_POOL_MAX=100              # Maximum number of connections in pool (was 20)
DB_POOL_MIN=10               # Minimum number of connections to maintain (was 5)
DB_IDLE_TIMEOUT=30000        # Time (ms) before idle connection is closed
DB_CONNECT_TIMEOUT=10000     # Time (ms) to wait for new connection
DB_STATEMENT_TIMEOUT=60000   # Time (ms) before query times out

# ============================================================================
# REDIS CACHE (OPTIONAL)
# ============================================================================

# Redis connection URL for caching (optional - app works fine without it)
# Leave empty to disable caching
# Format: redis://[username:password@]host:port[/database]
#
# Examples:
#   Local development:
#     redis://localhost:6379
#
#   Redis Cloud (free tier):
#     redis://default:YOUR_PASSWORD@redis-12345.c1.us-east-1-1.ec2.cloud.redislabs.com:12345
#
#   AWS ElastiCache:
#     redis://master.your-cluster.cache.amazonaws.com:6379
#
#   Redis with auth:
#     redis://username:password@localhost:6379
#
REDIS_URL=

# ============================================================================
# AUTHENTICATION
# ============================================================================

# Authentication mode: 'dev' or 'jwt'
# - dev: Uses X-User-Id headers for local development (no auth required)
# - jwt: Production mode with Clerk JWT verification
AUTH_MODE=dev

# Clerk JWT Configuration (required when AUTH_MODE=jwt)
# Get these from: https://dashboard.clerk.com -> API Keys -> Advanced -> JWT public key
#
# Issuer URL (iss claim) - your Clerk domain
AUTH_JWT_ISS=https://your-clerk-domain.clerk.accounts.dev

# Audience (aud claim) - optional, leave empty if not configured
AUTH_JWT_AUD=

# JWKS URL for public key verification
AUTH_JWT_JWKS_URL=https://your-clerk-domain.clerk.accounts.dev/.well-known/jwks.json

# ============================================================================
# LLM PROVIDER CONFIGURATION
# ============================================================================

# LLM provider: 'mock', 'gemini', or 'openai'
# - mock: Deterministic responses for testing (no API key required)
# - gemini: Google Gemini models (recommended)
# - openai: OpenAI GPT models
LLM_PROVIDER=mock

# ============================================================================
# GOOGLE GEMINI CONFIGURATION
# ============================================================================

# Gemini API Key (required when LLM_PROVIDER=gemini)
# Get from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=

# Gemini text generation model
# Options: gemini-1.5-flash (fast), gemini-1.5-pro (high quality), gemini-2.0-flash-exp
GEMINI_GEN_MODEL=gemini-2.5-flash

# Gemini vision model for PDF extraction
# Options: gemini-1.5-flash, gemini-1.5-pro
GEMINI_VISION_MODEL=gemini-2.5-flash-lite

# Temperature for vision model (0.0-1.0, lower = more deterministic)
GEMINI_VISION_TEMPERATURE=0.1

# ============================================================================
# OPENAI CONFIGURATION
# ============================================================================

# OpenAI API Key (required when LLM_PROVIDER=openai)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# ============================================================================
# PDF EXTRACTION CONFIGURATION
# ============================================================================

# PDF extraction mode
# - enhanced: Rich content extraction with formulas, diagrams, tables (recommended)
# - standard: Fast text-only extraction
# - auto: Automatically choose based on content (default)
# See: backend/PDF_EXTRACTION_GUIDE.md
PDF_EXTRACTION_MODE=enhanced

# ============================================================================
# JOB QUEUE CONFIGURATION
# ============================================================================

# Disable job queues entirely (for testing/debugging)
# Set to 'true' to process jobs synchronously
DISABLE_QUEUES=false

# Upload queue concurrency (number of parallel PDF uploads)
UPLOAD_QUEUE_CONCURRENCY=5

# Lesson generation queue concurrency (number of parallel lesson generations)
LESSON_QUEUE_CONCURRENCY=3

# Rate limiting for upload jobs (jobs per minute)
# Prevents overwhelming the LLM API
UPLOAD_JOBS_PER_MINUTE=60

# Rate limiting for lesson generation jobs (jobs per minute)
LESSON_JOBS_PER_MINUTE=60

# ============================================================================
# CACHING CONFIGURATION
# ============================================================================

# Enable lesson caching (requires Redis)
# Caches generated lessons to reduce LLM API calls
ENABLE_LESSON_CACHE=false

# Lesson cache TTL (time to live in seconds)
# How long to keep cached lessons (default: 1 hour)
LESSON_CACHE_TTL=3600

# ============================================================================
# RATE LIMITING
# ============================================================================

# Enable API rate limiting (optional, for production)
# Uses in-memory rate limiting (for single server)
# For multi-server deployments, use Redis-based rate limiting
ENABLE_RATE_LIMITING=false

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log level: DEBUG, INFO, WARN, ERROR
# - DEBUG: All logs including debug messages
# - INFO: Informational messages and above
# - WARN: Warnings and errors only
# - ERROR: Errors only
LOG_LEVEL=INFO

# ============================================================================
# AWS S3 CONFIGURATION (OPTIONAL - FOR PRODUCTION SCALING)
# ============================================================================
# See: SCALABILITY_GUIDE.md for setup instructions

# AWS credentials for S3 file storage
# AWS_ACCESS_KEY_ID=AKIA...
# AWS_SECRET_ACCESS_KEY=...

# S3 bucket for PDF storage
# AWS_S3_BUCKET=ultudy-pdfs-production

# AWS region
# AWS_REGION=us-east-1

# ============================================================================
# ADVANCED DATABASE CONFIGURATION (FOR READ REPLICAS)
# ============================================================================
# See: SCALABILITY_GUIDE.md for setup instructions

# Read replica database URL (for horizontal scaling)
# Routes read queries to replica for better performance
# DATABASE_REPLICA_URL=postgres://postgres:postgres@replica-host:5432/study_app

# ============================================================================
# NOTES
# ============================================================================
#
# Development Setup:
# 1. Copy this file: cp .env.example .env
# 2. Set GEMINI_API_KEY (or use LLM_PROVIDER=mock for testing)
# 3. Configure DATABASE_URL (or use default local postgres)
# 4. Keep AUTH_MODE=dev for local development
#
# Production Setup:
# 1. Set NODE_ENV=production
# 2. Set AUTH_MODE=jwt and configure Clerk credentials
# 3. Set up Redis with REDIS_URL for caching
# 4. Configure database connection pool (DB_POOL_MAX=100 minimum)
# 5. Set up proper DATABASE_URL with strong credentials
# 6. Enable ENABLE_RATE_LIMITING=true
# 7. Set LOG_LEVEL=WARN or ERROR
#
# Scaling to 10k+ Users:
# 1. Set up PgBouncer for database connection pooling
# 2. Set up Redis cluster for caching
# 3. Add read replicas with DATABASE_REPLICA_URL
# 4. Set up S3 for file storage (AWS_* variables)
# 5. Deploy multiple app instances behind load balancer
#
# For complete documentation, see:
# - backend/ENV_CONFIGURATION.md
# - SCALABILITY_GUIDE.md
# - backend/PDF_EXTRACTION_GUIDE.md
#
# ============================================================================
